This is a Research paper on Deep learning's impact on Speech Synthesis for Mobile devices.

While the first Voice assistant was built on a computer and was later explored into the world of Mobile devices, it has its own reasons, like limited memory resources, low or limited processing capacity, and the characteristic limitation of real-time responsiveness.
To tackle these challenges, researchers have investigated the use of deep learning models to improve voice quality, reduce memory footprint, and optimize embedded implementation in HMM-based synthesis systems.

HMM models prove to be the best approach for using speech synthesis on memory-constrained devices after a series of experiments and comparisons between the other three models.
Formant-based
Unit selection
HMM-based

• Limitations observed in HMM-based speech synthesis:
• The vocoder
• The precision of acoustic models
• The presence of the over-smoothing effect


Approaches to optimize HMM-based TTS:
• Adjusting vocoder parameters
• Reducing the size of decision trees
• Introducing streaming synthesis
• Applying source code optimizations



Lowering the number of nodes of a decision tree: Lower the quality of synthesized speech and Lower the memory footprint and the computational costs.

DNN-based approach improved performance and resulted in a more natural-sounding voice.

In this paper, a systematic review is presented focusing on the impact of deep learning, and different approaches to improving conventional HMM-based synthesis are pointed out.

We concluded that DNN can be implemented to reduce the footprint size and seamless operation on mobile devices and enable offline use of speech synthesis applications.

THIS PAPER WAS PRESENTED AT THE ICSISCET'2023 AND ACCEPTED TO BE PUBLISHED AT ALGORITHMS FOR INTELLIGENT SYSTEMS----- A SPRINGER JOURNAL
